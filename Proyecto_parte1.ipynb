{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b367eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\documents\\maestria inteligencia artifical\\trimestre 2\\proyecto luis\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9ed1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375a5898",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02ae4b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>passenger_class</th>\n",
       "      <th>passenger_sex</th>\n",
       "      <th>passenger_survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId                                               Name   Age  \\\n",
       "0            1                            Braund, Mr. Owen Harris  22.0   \n",
       "1            2  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0   \n",
       "2            3                             Heikkinen, Miss. Laina  26.0   \n",
       "3            4       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0   \n",
       "4            5                           Allen, Mr. William Henry  35.0   \n",
       "\n",
       "   SibSp  Parch            Ticket     Fare Cabin Embarked passenger_class  \\\n",
       "0      1      0         A/5 21171   7.2500   NaN        S           Lower   \n",
       "1      1      0          PC 17599  71.2833   C85        C           Upper   \n",
       "2      0      0  STON/O2. 3101282   7.9250   NaN        S           Lower   \n",
       "3      1      0            113803  53.1000  C123        S           Upper   \n",
       "4      0      0            373450   8.0500   NaN        S           Lower   \n",
       "\n",
       "  passenger_sex passenger_survived  \n",
       "0             M                  N  \n",
       "1             F                  Y  \n",
       "2             F                  Y  \n",
       "3             F                  Y  \n",
       "4             M                  N  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.read_csv('data_titanic_proyecto.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "638f2cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   PassengerId         891 non-null    int64  \n",
      " 1   Name                891 non-null    object \n",
      " 2   Age                 714 non-null    float64\n",
      " 3   SibSp               891 non-null    int64  \n",
      " 4   Parch               891 non-null    int64  \n",
      " 5   Ticket              891 non-null    object \n",
      " 6   Fare                891 non-null    float64\n",
      " 7   Cabin               204 non-null    object \n",
      " 8   Embarked            889 non-null    object \n",
      " 9   passenger_class     891 non-null    object \n",
      " 10  passenger_sex       891 non-null    object \n",
      " 11  passenger_survived  891 non-null    object \n",
      "dtypes: float64(2), int64(3), object(7)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a4b6cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId         Age       SibSp       Parch        Fare\n",
       "count   891.000000  714.000000  891.000000  891.000000  891.000000\n",
       "mean    446.000000   29.699118    0.523008    0.381594   32.204208\n",
       "std     257.353842   14.526497    1.102743    0.806057   49.693429\n",
       "min       1.000000    0.420000    0.000000    0.000000    0.000000\n",
       "25%     223.500000   20.125000    0.000000    0.000000    7.910400\n",
       "50%     446.000000   28.000000    0.000000    0.000000   14.454200\n",
       "75%     668.500000   38.000000    1.000000    0.000000   31.000000\n",
       "max     891.000000   80.000000    8.000000    6.000000  512.329200"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb6db256",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Age\"] < 1] = df[df[\"Age\"] < 1] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0fda0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Alternativa usando str.extract en lugar de apply + función\n",
    "df['TicketPrefix_alt'] = df['Ticket'].str.extract(r'([A-Za-z./]+)', expand=False).str.strip().fillna(\"NoPrefix\")\n",
    "df['TicketPrefix_alt'] = df['TicketPrefix_alt'].str.replace(r'\\W+', '_', regex=True).str.strip('_')\n",
    "df.drop(columns=['Ticket'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1dec61bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna Age tiene 177 valores faltantes. (19.87%)\n",
      "Columna Cabin tiene 687 valores faltantes. (77.10%)\n",
      "Columna Embarked tiene 2 valores faltantes. (0.22%)\n"
     ]
    }
   ],
   "source": [
    "for col, val in df.isna().sum().items():\n",
    "    if val > 0:\n",
    "        print(f\"Columna {col} tiene {val} valores faltantes. ({val/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82ec9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "cols_for_knn = [col for col in df.columns if col != 'Cabin']\n",
    "\n",
    "numeric_cols = df[cols_for_knn].select_dtypes(include=[np.number]).columns\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "df = df.drop(columns=['Cabin', 'Name'])\n",
    "df = df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e72e614e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>passenger_class</th>\n",
       "      <th>passenger_sex</th>\n",
       "      <th>passenger_survived</th>\n",
       "      <th>TicketPrefix_alt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Lower</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>STON_O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>NoPrefix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>NoPrefix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId   Age  SibSp  Parch     Fare passenger_class passenger_sex  \\\n",
       "0          1.0  22.0    1.0    0.0   7.2500           Lower             M   \n",
       "1          2.0  38.0    1.0    0.0  71.2833           Upper             F   \n",
       "2          3.0  26.0    0.0    0.0   7.9250           Lower             F   \n",
       "3          4.0  35.0    1.0    0.0  53.1000           Upper             F   \n",
       "4          5.0  35.0    0.0    0.0   8.0500           Lower             M   \n",
       "\n",
       "  passenger_survived TicketPrefix_alt  \n",
       "0                  N                A  \n",
       "1                  Y               PC  \n",
       "2                  Y           STON_O  \n",
       "3                  Y         NoPrefix  \n",
       "4                  N         NoPrefix  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "747e2615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>485.313131</td>\n",
       "      <td>29.782604</td>\n",
       "      <td>0.593715</td>\n",
       "      <td>0.472503</td>\n",
       "      <td>34.838888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>569.224167</td>\n",
       "      <td>13.204739</td>\n",
       "      <td>1.536421</td>\n",
       "      <td>1.420586</td>\n",
       "      <td>71.372436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>224.500000</td>\n",
       "      <td>21.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>448.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>672.500000</td>\n",
       "      <td>36.550000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8320.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1515.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId         Age       SibSp       Parch         Fare\n",
       "count   891.000000  891.000000  891.000000  891.000000   891.000000\n",
       "mean    485.313131   29.782604    0.593715    0.472503    34.838888\n",
       "std     569.224167   13.204739    1.536421    1.420586    71.372436\n",
       "min       1.000000    1.000000    0.000000    0.000000     0.000000\n",
       "25%     224.500000   21.700000    0.000000    0.000000     7.910400\n",
       "50%     448.000000   29.000000    0.000000    0.000000    14.454200\n",
       "75%     672.500000   36.550000    1.000000    0.000000    31.275000\n",
       "max    8320.000000   80.000000   20.000000   20.000000  1515.500000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1dad2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, val in df.isna().sum().items():\n",
    "    if val > 0:\n",
    "        print(f\"Columna {col} tiene {val} valores faltantes. ({val/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c7c18bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>passenger_class</th>\n",
       "      <th>passenger_sex</th>\n",
       "      <th>passenger_survived</th>\n",
       "      <th>TicketPrefix_alt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Lower</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>STON_O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>NoPrefix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>NoPrefix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId   Age  SibSp  Parch     Fare passenger_class passenger_sex  \\\n",
       "0          1.0  22.0    1.0    0.0   7.2500           Lower             M   \n",
       "1          2.0  38.0    1.0    0.0  71.2833           Upper             F   \n",
       "2          3.0  26.0    0.0    0.0   7.9250           Lower             F   \n",
       "3          4.0  35.0    1.0    0.0  53.1000           Upper             F   \n",
       "4          5.0  35.0    0.0    0.0   8.0500           Lower             M   \n",
       "\n",
       "  passenger_survived TicketPrefix_alt  \n",
       "0                  N                A  \n",
       "1                  Y               PC  \n",
       "2                  Y           STON_O  \n",
       "3                  Y         NoPrefix  \n",
       "4                  N         NoPrefix  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78fa3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_discretas = []\n",
    "col_categoricas = []\n",
    "col_continuas = []\n",
    "for col in df.columns:\n",
    "    if(df[col].dtype == 'object') or (df[col].dtype == 'category'):\n",
    "        col_categoricas.append(col)\n",
    "    else:\n",
    "        if (df[col].nunique() < 20):\n",
    "            col_discretas.append(col)\n",
    "        else:\n",
    "            col_continuas.append(col)\n",
    "\n",
    "col_continuas.remove(\"PassengerId\")\n",
    "col_categoricas.remove(\"passenger_survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01be3f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"passenger_sex\"] = df[\"passenger_sex\"].str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78a2fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['passenger_class'].str.contains(\"Lower\", case=False, na=False), 'passenger_class'] = \"Lower\"\n",
    "df.loc[df['passenger_class'].str.contains(\"Middle\", case=False, na=False), 'passenger_class'] = \"Middle\"\n",
    "df.loc[df['passenger_class'].str.contains(\"Upper\", case=False, na=False), 'passenger_class'] = \"Upper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65bb42a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_30292\\3585606886.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['passenger_survived'] = df['passenger_survived'].replace({'Y': 1, 'N': 0})\n"
     ]
    }
   ],
   "source": [
    "df['passenger_survived'] = df['passenger_survived'].astype(str).str.strip().str[0].str.upper()\n",
    "df['passenger_survived'] = df['passenger_survived'].replace({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51c7be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[col_continuas] = scaler.fit_transform(df[col_continuas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "688946cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df, columns=col_categoricas, drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3dd50767",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(columns=['PassengerId'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f310ed96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>passenger_survived</th>\n",
       "      <th>passenger_class_Lower</th>\n",
       "      <th>passenger_class_Middle</th>\n",
       "      <th>passenger_class_Upper</th>\n",
       "      <th>passenger_sex_F</th>\n",
       "      <th>passenger_sex_M</th>\n",
       "      <th>...</th>\n",
       "      <th>TicketPrefix_alt_S_C_A</th>\n",
       "      <th>TicketPrefix_alt_S_C_PARIS</th>\n",
       "      <th>TicketPrefix_alt_S_O_C</th>\n",
       "      <th>TicketPrefix_alt_S_O_P</th>\n",
       "      <th>TicketPrefix_alt_S_O_P_P</th>\n",
       "      <th>TicketPrefix_alt_S_P</th>\n",
       "      <th>TicketPrefix_alt_S_W_PP</th>\n",
       "      <th>TicketPrefix_alt_WE_P</th>\n",
       "      <th>TicketPrefix_alt_W_C</th>\n",
       "      <th>TicketPrefix_alt_W_E_P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.265823</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004784</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.468354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047036</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.316456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.430380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035038</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005312</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age  SibSp  Parch      Fare  passenger_survived  \\\n",
       "0  0.265823    1.0    0.0  0.004784                   0   \n",
       "1  0.468354    1.0    0.0  0.047036                   1   \n",
       "2  0.316456    0.0    0.0  0.005229                   1   \n",
       "3  0.430380    1.0    0.0  0.035038                   1   \n",
       "4  0.430380    0.0    0.0  0.005312                   0   \n",
       "\n",
       "   passenger_class_Lower  passenger_class_Middle  passenger_class_Upper  \\\n",
       "0                   True                   False                  False   \n",
       "1                  False                   False                   True   \n",
       "2                   True                   False                  False   \n",
       "3                  False                   False                   True   \n",
       "4                   True                   False                  False   \n",
       "\n",
       "   passenger_sex_F  passenger_sex_M  ...  TicketPrefix_alt_S_C_A  \\\n",
       "0            False             True  ...                   False   \n",
       "1             True            False  ...                   False   \n",
       "2             True            False  ...                   False   \n",
       "3             True            False  ...                   False   \n",
       "4            False             True  ...                   False   \n",
       "\n",
       "   TicketPrefix_alt_S_C_PARIS  TicketPrefix_alt_S_O_C  TicketPrefix_alt_S_O_P  \\\n",
       "0                       False                   False                   False   \n",
       "1                       False                   False                   False   \n",
       "2                       False                   False                   False   \n",
       "3                       False                   False                   False   \n",
       "4                       False                   False                   False   \n",
       "\n",
       "   TicketPrefix_alt_S_O_P_P  TicketPrefix_alt_S_P  TicketPrefix_alt_S_W_PP  \\\n",
       "0                     False                 False                    False   \n",
       "1                     False                 False                    False   \n",
       "2                     False                 False                    False   \n",
       "3                     False                 False                    False   \n",
       "4                     False                 False                    False   \n",
       "\n",
       "   TicketPrefix_alt_WE_P  TicketPrefix_alt_W_C  TicketPrefix_alt_W_E_P  \n",
       "0                  False                 False                   False  \n",
       "1                  False                 False                   False  \n",
       "2                  False                 False                   False  \n",
       "3                  False                 False                   False  \n",
       "4                  False                 False                   False  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7dda63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'SibSp', 'Parch', 'Fare', 'passenger_survived',\n",
       "       'passenger_class_Lower', 'passenger_class_Middle',\n",
       "       'passenger_class_Upper', 'passenger_sex_F', 'passenger_sex_M',\n",
       "       'TicketPrefix_alt_A', 'TicketPrefix_alt_A_S', 'TicketPrefix_alt_C',\n",
       "       'TicketPrefix_alt_CA', 'TicketPrefix_alt_C_A',\n",
       "       'TicketPrefix_alt_C_A_SOTON', 'TicketPrefix_alt_F_C',\n",
       "       'TicketPrefix_alt_F_C_C', 'TicketPrefix_alt_Fa',\n",
       "       'TicketPrefix_alt_LINE', 'TicketPrefix_alt_NoPrefix',\n",
       "       'TicketPrefix_alt_PC', 'TicketPrefix_alt_PP', 'TicketPrefix_alt_P_PP',\n",
       "       'TicketPrefix_alt_SC', 'TicketPrefix_alt_SCO_W',\n",
       "       'TicketPrefix_alt_SC_AH', 'TicketPrefix_alt_SC_PARIS',\n",
       "       'TicketPrefix_alt_SC_Paris', 'TicketPrefix_alt_SOTON_O',\n",
       "       'TicketPrefix_alt_SOTON_OQ', 'TicketPrefix_alt_SOTON_O_Q',\n",
       "       'TicketPrefix_alt_SO_C', 'TicketPrefix_alt_STON_O',\n",
       "       'TicketPrefix_alt_SW_PP', 'TicketPrefix_alt_S_C_A',\n",
       "       'TicketPrefix_alt_S_C_PARIS', 'TicketPrefix_alt_S_O_C',\n",
       "       'TicketPrefix_alt_S_O_P', 'TicketPrefix_alt_S_O_P_P',\n",
       "       'TicketPrefix_alt_S_P', 'TicketPrefix_alt_S_W_PP',\n",
       "       'TicketPrefix_alt_WE_P', 'TicketPrefix_alt_W_C',\n",
       "       'TicketPrefix_alt_W_E_P'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ee29f",
   "metadata": {},
   "source": [
    "# SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95376893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (534, 45)\n",
      "Validate shape: (178, 45)\n",
      "Test shape: (179, 45)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_temp = train_test_split(df_encoded, test_size=0.4, random_state=42)\n",
    "df_validate, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "df_test.to_csv('test.csv', index=False)\n",
    "\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Validate shape: {df_validate.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f62397",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e71c67",
   "metadata": {},
   "source": [
    "## Arbol de Desicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "92aef2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def train_decision_tree(X_train, y_train, X_test, y_test, max_depth, experiment_log=\"bitacora.csv\"):\n",
    "\n",
    "    model = DecisionTreeClassifier(random_state=42, max_depth=max_depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    \n",
    "    experiment_name = f\"decisiontree_{max_depth}\"\n",
    "    \n",
    "    joblib.dump(model, f\"{experiment_name}.pkl\")\n",
    "\n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"Model: {experiment_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    result = {\n",
    "        \"model\": \"Decision Tree\",\n",
    "        \"model_file\": f\"{experiment_name}.pkl\",\n",
    "        \"variables\": {\n",
    "            \"max_depth\": max_depth\n",
    "        },\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": report_dict[\"weighted avg\"][\"precision\"],\n",
    "        \"recall\": report_dict[\"weighted avg\"][\"recall\"],\n",
    "        \"f1_score\": report_dict[\"weighted avg\"][\"f1-score\"]\n",
    "    }\n",
    "    \n",
    "    df_result = pd.DataFrame([result])\n",
    "    if os.path.exists(experiment_log):\n",
    "        df_result.to_csv(experiment_log, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_result.to_csv(experiment_log, index=False)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f843c1a",
   "metadata": {},
   "source": [
    "## SMV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2379129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "def train_svm(X_train, y_train, X_test, y_test, C=1.0, kernel='rbf', gamma='scale', experiment_log=\"bitacora.csv\"):\n",
    "    # Entrenamiento del modelo SVM\n",
    "    model = SVC(C=C, kernel=kernel, gamma=gamma, probability=True, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predicción\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Mostrar resultados\n",
    "\n",
    "    # Nombre del experimento\n",
    "    experiment_name = f\"svm_C={C}_kernel={kernel}_gamma={gamma}\"\n",
    "\n",
    "    # Guardar modelo\n",
    "    joblib.dump(model, f\"{experiment_name}.pkl\")\n",
    "\n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"Model: {experiment_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Guardar en bitácora\n",
    "    result = {\n",
    "        \"model\": \"SVM\",\n",
    "        \"model_file\": f\"{experiment_name}.pkl\",\n",
    "        \"variables\": {\n",
    "            \"C\": C,\n",
    "            \"kernel\": kernel,\n",
    "            \"gamma\": gamma\n",
    "        },\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": report_dict[\"weighted avg\"][\"precision\"],\n",
    "        \"recall\": report_dict[\"weighted avg\"][\"recall\"],\n",
    "        \"f1_score\": report_dict[\"weighted avg\"][\"f1-score\"]\n",
    "    }\n",
    "    \n",
    "\n",
    "    df_result = pd.DataFrame([result])\n",
    "    if os.path.exists(experiment_log):\n",
    "        df_result.to_csv(experiment_log, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_result.to_csv(experiment_log, index=False)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d17147",
   "metadata": {},
   "source": [
    "## Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4277f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import os\n",
    "\n",
    "def train_naive_bayes_vectorized(X_train, y_train, X_test, y_test, laplace=1, experiment_log=\"bitacora.csv\"):\n",
    "    # Combinar X e y\n",
    "    df_train = X_train.copy()\n",
    "    df_train['target'] = y_train\n",
    "    \n",
    "    clases = y_train.unique()\n",
    "    features = X_train.columns\n",
    "\n",
    "    # Calcular probabilidades a priori\n",
    "    class_counts = df_train['target'].value_counts()\n",
    "    priors = (class_counts / len(df_train)).to_dict()\n",
    "\n",
    "    # Probabilidades condicionales (likelihoods) con Laplace smoothing\n",
    "    likelihoods = {}\n",
    "    for feature in features:\n",
    "        cond_counts = df_train.groupby([feature, 'target']).size().unstack(fill_value=0)\n",
    "        total_per_class = class_counts + laplace * len(cond_counts.index)\n",
    "        cond_probs = (cond_counts + laplace).div(total_per_class, axis=1)\n",
    "        likelihoods[feature] = cond_probs\n",
    "\n",
    "    # Función de predicción vectorizada\n",
    "    def predict_vectorized(X):\n",
    "        log_priors = {cls: np.log(p) for cls, p in priors.items()}\n",
    "        predictions = []\n",
    "\n",
    "        for _, row in X.iterrows():\n",
    "            log_posteriors = {}\n",
    "            for cls in clases:\n",
    "                log_posterior = log_priors[cls]\n",
    "                for feature in features:\n",
    "                    val = row[feature]\n",
    "                    prob = likelihoods[feature].get(cls).get(val, laplace / (class_counts[cls] + laplace * len(likelihoods[feature])))\n",
    "                    log_posterior += np.log(prob)\n",
    "                log_posteriors[cls] = log_posterior\n",
    "            predictions.append(max(log_posteriors, key=log_posteriors.get))\n",
    "        return pd.Series(predictions)\n",
    "\n",
    "    # Predicción\n",
    "    y_pred = predict_vectorized(X_test)\n",
    "\n",
    "    # Métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Guardar modelo\n",
    "    model_name = f\"naivebayesV_laplace={laplace}\"\n",
    "    combined = []\n",
    "    for feature in features:\n",
    "        df = likelihoods[feature].copy()\n",
    "        df.index.name = 'feature_value'\n",
    "        df.reset_index(inplace=True)\n",
    "        df['feature'] = feature\n",
    "        combined.append(df)\n",
    "\n",
    "    likelihoods_df = pd.concat(combined, ignore_index=True)\n",
    "    likelihoods_df.to_csv(f\"{model_name}_likelihoods.csv\", index=False)\n",
    "\n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Guardar bitácora\n",
    "    result = {\n",
    "        \"model\": \"Naive Bayes Vectorized\",\n",
    "        \"model_file\": f\"{model_name}\",\n",
    "        \"variables\": {\n",
    "            \"laplace\": laplace\n",
    "        },\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": report_dict[\"weighted avg\"][\"precision\"],\n",
    "        \"recall\": report_dict[\"weighted avg\"][\"recall\"],\n",
    "        \"f1_score\": report_dict[\"weighted avg\"][\"f1-score\"]\n",
    "    }\n",
    "    df_result = pd.DataFrame([result])\n",
    "    if os.path.exists(experiment_log):\n",
    "        df_result.to_csv(experiment_log, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_result.to_csv(experiment_log, index=False)\n",
    "\n",
    "    return model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06b87bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_naive_bayes_model(model_name):\n",
    "    df = pd.read_csv(f\"{model_name}_likelihoods.csv\")\n",
    "    \n",
    "    clases = [col for col in df.columns if col not in ['feature', 'feature_value']]\n",
    "    \n",
    "    likelihoods = {}\n",
    "    for feature, group in df.groupby('feature'):\n",
    "        sub_df = group.set_index('feature_value')[clases]\n",
    "        likelihoods[feature] = sub_df\n",
    "\n",
    "    primer_feature = list(likelihoods.keys())[0]\n",
    "    priors = likelihoods[primer_feature].sum().to_dict()\n",
    "    total = sum(priors.values())\n",
    "    priors = {k: v / total for k, v in priors.items()}\n",
    "\n",
    "    return priors, likelihoods\n",
    "\n",
    "def predict_naive_bayes(X, priors, likelihoods, laplace=1):\n",
    "    clases = list(priors.keys())\n",
    "    features = X.columns\n",
    "    predictions = []\n",
    "\n",
    "    for _, row in X.iterrows():\n",
    "        log_posteriors = {}\n",
    "        for cls in clases:\n",
    "            log_prob = np.log(priors[cls])\n",
    "            for feature in features:\n",
    "                val = row[feature]\n",
    "                tabla = likelihoods[feature]\n",
    "                if val in tabla.index and cls in tabla.columns:\n",
    "                    prob = tabla.at[val, cls]\n",
    "                else:\n",
    "                    # smoothing si el valor no fue visto\n",
    "                    prob = laplace / (laplace * len(tabla.index))  # Denominador estimado\n",
    "                log_prob += np.log(prob)\n",
    "            log_posteriors[cls] = log_prob\n",
    "        predictions.append(max(log_posteriors, key=log_posteriors.get))\n",
    "    return pd.Series(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ef2a5",
   "metadata": {},
   "source": [
    "## REG. LOG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4b3b244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import os\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = np.asarray(z, dtype=np.float64)  # Asegura compatibilidad con np.exp\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_loss_and_gradients(X, y, w, reg_type, lambda_):\n",
    "\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    y = np.asarray(y, dtype=np.float64).reshape(-1, 1)\n",
    "    w = np.asarray(w, dtype=np.float64)\n",
    "\n",
    "    m = len(y)\n",
    "    h = sigmoid(X @ w)\n",
    "    error = h - y\n",
    "\n",
    "    if reg_type == 'l2':\n",
    "        reg_term = lambda_ * np.sum(w[1:] ** 2) / (2 * m)\n",
    "        loss = -np.mean(y * np.log(h + 1e-9) + (1 - y) * np.log(1 - h + 1e-9)) + reg_term\n",
    "        grad = (X.T @ error) / m\n",
    "        grad[1:] += (lambda_ / m) * w[1:]\n",
    "\n",
    "    elif reg_type == 'l1':\n",
    "        reg_term = lambda_ * np.sum(np.abs(w[1:])) / m\n",
    "        loss = -np.mean(y * np.log(h + 1e-9) + (1 - y) * np.log(1 - h + 1e-9)) + reg_term\n",
    "        grad = (X.T @ error) / m\n",
    "        grad[1:] += (lambda_ / m) * np.sign(w[1:])\n",
    "\n",
    "    else:  # sin regularización\n",
    "        loss = -np.mean(y * np.log(h + 1e-9) + (1 - y) * np.log(1 - h + 1e-9))\n",
    "        grad = (X.T @ error) / m\n",
    "\n",
    "    return loss, grad\n",
    "\n",
    "def train_logistic_regression_numpy(X_train, y_train, X_val, y_val,\n",
    "                                     lr=0.01, lambda_=0.1, epochs=1000,\n",
    "                                     batch_size=32, reg_type='l2',\n",
    "                                     experiment_log='bitacora.csv'):\n",
    "    # Preparar datos\n",
    "    X_train = np.c_[np.ones(X_train.shape[0]), X_train]  # agregar bias\n",
    "    X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
    "    y_train = y_train.values.reshape(-1, 1)\n",
    "    y_val = y_val.values.reshape(-1, 1)\n",
    "\n",
    "    n_features = X_train.shape[1]\n",
    "    w = np.zeros((n_features, 1))\n",
    "\n",
    "    # Mini-batch training\n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            X_batch = X_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            loss, grad = compute_loss_and_gradients(X_batch, y_batch, w, reg_type, lambda_)\n",
    "            w -= lr * grad\n",
    "\n",
    "    # Validación\n",
    "    y_val_pred = sigmoid(X_val @ w) >= 0.5\n",
    "    y_val_pred = y_val_pred.astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    report_dict = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "\n",
    "\n",
    "    # Guardar modelo\n",
    "    reg_label = f\"{reg_type}Reg\" if reg_type in ['l1', 'l2'] else \"NoReg\"\n",
    "    model_name = f\"logreg_lr={lr}_lambda={lambda_}_batch={batch_size}_{reg_label}\"\n",
    "    np.save(f\"{model_name}.npy\", w)\n",
    "\n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Bitácora\n",
    "    result = {\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"model_file\": f\"{model_name}.npy\",\n",
    "        \"variables\": {\n",
    "            \"lr\": lr,\n",
    "            \"lambda\": lambda_,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"reg_type\": reg_type\n",
    "        },\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": report_dict[\"weighted avg\"][\"precision\"],\n",
    "        \"recall\": report_dict[\"weighted avg\"][\"recall\"],\n",
    "        \"f1_score\": report_dict[\"weighted avg\"][\"f1-score\"]\n",
    "    }\n",
    "    df_result = pd.DataFrame([result])\n",
    "    if os.path.exists(experiment_log):\n",
    "        df_result.to_csv(experiment_log, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_result.to_csv(experiment_log, index=False)\n",
    "\n",
    "    return w  # puedes retornar también model_name si quieres usarlo en inferencia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ae82181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logistic_regression(X, w, threshold=0.5):\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    \n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    \n",
    "    w = np.asarray(w, dtype=np.float64)\n",
    "    probs = sigmoid(X @ w)\n",
    "    return (probs >= threshold).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a8bcf2",
   "metadata": {},
   "source": [
    "# Entrenamientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3db3a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'passenger_survived'\n",
    "X_train = df_train.drop(columns=[target])\n",
    "y_train = df_train[target]\n",
    "X_val = df_validate.drop(columns=[target])\n",
    "y_val = df_validate[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7c232c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "Model: decisiontree_3\n",
      "Accuracy: 0.78\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       113\n",
      "           1       0.72      0.63      0.67        65\n",
      "\n",
      "    accuracy                           0.78       178\n",
      "   macro avg       0.76      0.74      0.75       178\n",
      "weighted avg       0.77      0.78      0.77       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[97 16]\n",
      " [24 41]]\n",
      "\n",
      "Validation Results:\n",
      "Model: decisiontree_4\n",
      "Accuracy: 0.78\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83       113\n",
      "           1       0.73      0.63      0.68        65\n",
      "\n",
      "    accuracy                           0.78       178\n",
      "   macro avg       0.77      0.75      0.76       178\n",
      "weighted avg       0.78      0.78      0.78       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[98 15]\n",
      " [24 41]]\n",
      "\n",
      "Validation Results:\n",
      "Model: decisiontree_5\n",
      "Accuracy: 0.77\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83       113\n",
      "           1       0.72      0.60      0.66        65\n",
      "\n",
      "    accuracy                           0.77       178\n",
      "   macro avg       0.76      0.73      0.74       178\n",
      "weighted avg       0.77      0.77      0.76       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[98 15]\n",
      " [26 39]]\n",
      "\n",
      "Validation Results:\n",
      "Model: decisiontree_6\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.88      0.82       113\n",
      "           1       0.72      0.52      0.61        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.74      0.70      0.71       178\n",
      "weighted avg       0.75      0.75      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[100  13]\n",
      " [ 31  34]]\n",
      "\n",
      "Validation Results:\n",
      "Model: decisiontree_7\n",
      "Accuracy: 0.76\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.82       113\n",
      "           1       0.71      0.57      0.63        65\n",
      "\n",
      "    accuracy                           0.76       178\n",
      "   macro avg       0.74      0.72      0.73       178\n",
      "weighted avg       0.75      0.76      0.75       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[98 15]\n",
      " [28 37]]\n",
      "\n",
      "Validation Results:\n",
      "Model: decisiontree_8\n",
      "Accuracy: 0.78\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       113\n",
      "           1       0.72      0.63      0.67        65\n",
      "\n",
      "    accuracy                           0.78       178\n",
      "   macro avg       0.76      0.74      0.75       178\n",
      "weighted avg       0.77      0.78      0.77       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[97 16]\n",
      " [24 41]]\n",
      "\n",
      "Validation Results:\n",
      "Model: decisiontree_9\n",
      "Accuracy: 0.78\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83       113\n",
      "           1       0.73      0.63      0.68        65\n",
      "\n",
      "    accuracy                           0.78       178\n",
      "   macro avg       0.77      0.75      0.76       178\n",
      "weighted avg       0.78      0.78      0.78       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[98 15]\n",
      " [24 41]]\n",
      "\n",
      "Validation Results:\n",
      "Model: decisiontree_10\n",
      "Accuracy: 0.76\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82       113\n",
      "           1       0.70      0.58      0.64        65\n",
      "\n",
      "    accuracy                           0.76       178\n",
      "   macro avg       0.74      0.72      0.73       178\n",
      "weighted avg       0.75      0.76      0.75       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[97 16]\n",
      " [27 38]]\n",
      "\n",
      "Validation Results:\n",
      "Model: decisiontree_11\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81       113\n",
      "           1       0.69      0.57      0.62        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.71      0.72       178\n",
      "weighted avg       0.74      0.75      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[96 17]\n",
      " [28 37]]\n",
      "\n",
      "Validation Results:\n",
      "Model: decisiontree_12\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.81       113\n",
      "           1       0.69      0.58      0.63        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.74      0.72      0.72       178\n",
      "weighted avg       0.75      0.75      0.75       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[96 17]\n",
      " [27 38]]\n",
      "\n",
      "Validation Results:\n",
      "Model: svm_C=0.1_kernel=rbf_gamma=scale\n",
      "Accuracy: 0.78\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.83       113\n",
      "           1       0.74      0.60      0.66        65\n",
      "\n",
      "    accuracy                           0.78       178\n",
      "   macro avg       0.76      0.74      0.75       178\n",
      "weighted avg       0.77      0.78      0.77       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[99 14]\n",
      " [26 39]]\n",
      "\n",
      "Validation Results:\n",
      "Model: svm_C=0.5_kernel=rbf_gamma=scale\n",
      "Accuracy: 0.79\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       113\n",
      "           1       0.75      0.62      0.68        65\n",
      "\n",
      "    accuracy                           0.79       178\n",
      "   macro avg       0.78      0.75      0.76       178\n",
      "weighted avg       0.78      0.79      0.78       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[100  13]\n",
      " [ 25  40]]\n",
      "\n",
      "Validation Results:\n",
      "Model: svm_C=1_kernel=rbf_gamma=scale\n",
      "Accuracy: 0.79\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       113\n",
      "           1       0.75      0.62      0.68        65\n",
      "\n",
      "    accuracy                           0.79       178\n",
      "   macro avg       0.78      0.75      0.76       178\n",
      "weighted avg       0.78      0.79      0.78       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[100  13]\n",
      " [ 25  40]]\n",
      "\n",
      "Validation Results:\n",
      "Model: svm_C=2_kernel=rbf_gamma=scale\n",
      "Accuracy: 0.78\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.84       113\n",
      "           1       0.75      0.60      0.67        65\n",
      "\n",
      "    accuracy                           0.78       178\n",
      "   macro avg       0.77      0.74      0.75       178\n",
      "weighted avg       0.78      0.78      0.77       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[100  13]\n",
      " [ 26  39]]\n",
      "\n",
      "Validation Results:\n",
      "Model: svm_C=5_kernel=rbf_gamma=scale\n",
      "Accuracy: 0.76\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82       113\n",
      "           1       0.71      0.60      0.65        65\n",
      "\n",
      "    accuracy                           0.76       178\n",
      "   macro avg       0.75      0.73      0.74       178\n",
      "weighted avg       0.76      0.76      0.76       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[97 16]\n",
      " [26 39]]\n",
      "\n",
      "Validation Results:\n",
      "Model: svm_C=10_kernel=rbf_gamma=scale\n",
      "Accuracy: 0.76\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82       113\n",
      "           1       0.70      0.58      0.64        65\n",
      "\n",
      "    accuracy                           0.76       178\n",
      "   macro avg       0.74      0.72      0.73       178\n",
      "weighted avg       0.75      0.76      0.75       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[97 16]\n",
      " [27 38]]\n",
      "\n",
      "Validation Results:\n",
      "Model: svm_C=20_kernel=rbf_gamma=scale\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.81       113\n",
      "           1       0.67      0.60      0.63        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.72      0.72       178\n",
      "weighted avg       0.74      0.75      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[94 19]\n",
      " [26 39]]\n",
      "\n",
      "Validation Results:\n",
      "Model: svm_C=50_kernel=rbf_gamma=scale\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.81       113\n",
      "           1       0.67      0.60      0.63        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.72      0.72       178\n",
      "weighted avg       0.74      0.75      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[94 19]\n",
      " [26 39]]\n",
      "\n",
      "Validation Results:\n",
      "Model: svm_C=100_kernel=rbf_gamma=scale\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81       113\n",
      "           1       0.68      0.62      0.65        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.72      0.73       178\n",
      "weighted avg       0.75      0.75      0.75       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[94 19]\n",
      " [25 40]]\n",
      "\n",
      "Validation Results:\n",
      "Model: svm_C=200_kernel=rbf_gamma=scale\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.84      0.81       113\n",
      "           1       0.68      0.58      0.63        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.71      0.72       178\n",
      "weighted avg       0.74      0.75      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[95 18]\n",
      " [27 38]]\n",
      "\n",
      "Validation Results:\n",
      "Model: naivebayesV_laplace=1\n",
      "Accuracy: 0.76\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.81       113\n",
      "           1       0.66      0.71      0.68        65\n",
      "\n",
      "    accuracy                           0.76       178\n",
      "   macro avg       0.74      0.75      0.74       178\n",
      "weighted avg       0.76      0.76      0.76       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[89 24]\n",
      " [19 46]]\n",
      "\n",
      "Validation Results:\n",
      "Model: naivebayesV_laplace=2\n",
      "Accuracy: 0.74\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79       113\n",
      "           1       0.63      0.68      0.65        65\n",
      "\n",
      "    accuracy                           0.74       178\n",
      "   macro avg       0.72      0.72      0.72       178\n",
      "weighted avg       0.74      0.74      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[87 26]\n",
      " [21 44]]\n",
      "\n",
      "Validation Results:\n",
      "Model: naivebayesV_laplace=3\n",
      "Accuracy: 0.74\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79       113\n",
      "           1       0.63      0.66      0.65        65\n",
      "\n",
      "    accuracy                           0.74       178\n",
      "   macro avg       0.72      0.72      0.72       178\n",
      "weighted avg       0.74      0.74      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[88 25]\n",
      " [22 43]]\n",
      "\n",
      "Validation Results:\n",
      "Model: naivebayesV_laplace=4\n",
      "Accuracy: 0.74\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80       113\n",
      "           1       0.65      0.65      0.65        65\n",
      "\n",
      "    accuracy                           0.74       178\n",
      "   macro avg       0.72      0.72      0.72       178\n",
      "weighted avg       0.74      0.74      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[90 23]\n",
      " [23 42]]\n",
      "\n",
      "Validation Results:\n",
      "Model: naivebayesV_laplace=5\n",
      "Accuracy: 0.74\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80       113\n",
      "           1       0.65      0.63      0.64        65\n",
      "\n",
      "    accuracy                           0.74       178\n",
      "   macro avg       0.72      0.72      0.72       178\n",
      "weighted avg       0.74      0.74      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[91 22]\n",
      " [24 41]]\n",
      "\n",
      "Validation Results:\n",
      "Model: naivebayesV_laplace=6\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.81       113\n",
      "           1       0.67      0.63      0.65        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.73      0.73       178\n",
      "weighted avg       0.75      0.75      0.75       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[93 20]\n",
      " [24 41]]\n",
      "\n",
      "Validation Results:\n",
      "Model: naivebayesV_laplace=7\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.81       113\n",
      "           1       0.67      0.62      0.64        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.72      0.72       178\n",
      "weighted avg       0.74      0.75      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[93 20]\n",
      " [25 40]]\n",
      "\n",
      "Validation Results:\n",
      "Model: naivebayesV_laplace=8\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.81       113\n",
      "           1       0.68      0.60      0.64        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.72      0.73       178\n",
      "weighted avg       0.75      0.75      0.75       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[95 18]\n",
      " [26 39]]\n",
      "\n",
      "Validation Results:\n",
      "Model: naivebayesV_laplace=9\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.84      0.81       113\n",
      "           1       0.68      0.58      0.63        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.71      0.72       178\n",
      "weighted avg       0.74      0.75      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[95 18]\n",
      " [27 38]]\n",
      "\n",
      "Validation Results:\n",
      "Model: naivebayesV_laplace=10\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81       113\n",
      "           1       0.69      0.57      0.62        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.71      0.72       178\n",
      "weighted avg       0.74      0.75      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[96 17]\n",
      " [28 37]]\n",
      "\n",
      "Validation Results:\n",
      "Model: logreg_lr=0.01_lambda=0.001_batch=32_l2Reg\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.81       113\n",
      "           1       0.67      0.62      0.64        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.72      0.72       178\n",
      "weighted avg       0.74      0.75      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[93 20]\n",
      " [25 40]]\n",
      "\n",
      "Validation Results:\n",
      "Model: logreg_lr=0.01_lambda=0.01_batch=32_l2Reg\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.81       113\n",
      "           1       0.67      0.62      0.64        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.72      0.72       178\n",
      "weighted avg       0.74      0.75      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[93 20]\n",
      " [25 40]]\n",
      "\n",
      "Validation Results:\n",
      "Model: logreg_lr=0.01_lambda=0.05_batch=32_l2Reg\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81       113\n",
      "           1       0.68      0.62      0.65        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.72      0.73       178\n",
      "weighted avg       0.75      0.75      0.75       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[94 19]\n",
      " [25 40]]\n",
      "\n",
      "Validation Results:\n",
      "Model: logreg_lr=0.01_lambda=0.1_batch=32_l2Reg\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81       113\n",
      "           1       0.68      0.62      0.65        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.72      0.73       178\n",
      "weighted avg       0.75      0.75      0.75       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[94 19]\n",
      " [25 40]]\n",
      "\n",
      "Validation Results:\n",
      "Model: logreg_lr=0.01_lambda=0.2_batch=32_l2Reg\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81       113\n",
      "           1       0.68      0.62      0.65        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.72      0.73       178\n",
      "weighted avg       0.75      0.75      0.75       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[94 19]\n",
      " [25 40]]\n",
      "\n",
      "Validation Results:\n",
      "Model: logreg_lr=0.01_lambda=0.5_batch=32_l2Reg\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81       113\n",
      "           1       0.68      0.62      0.65        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.72      0.73       178\n",
      "weighted avg       0.75      0.75      0.75       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[94 19]\n",
      " [25 40]]\n",
      "\n",
      "Validation Results:\n",
      "Model: logreg_lr=0.01_lambda=1_batch=32_l2Reg\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81       113\n",
      "           1       0.68      0.62      0.65        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.72      0.73       178\n",
      "weighted avg       0.75      0.75      0.75       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[94 19]\n",
      " [25 40]]\n",
      "\n",
      "Validation Results:\n",
      "Model: logreg_lr=0.01_lambda=2_batch=32_l2Reg\n",
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.81       113\n",
      "           1       0.70      0.54      0.61        65\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.73      0.70      0.71       178\n",
      "weighted avg       0.74      0.75      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[98 15]\n",
      " [30 35]]\n",
      "\n",
      "Validation Results:\n",
      "Model: logreg_lr=0.01_lambda=5_batch=32_l2Reg\n",
      "Accuracy: 0.76\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.94      0.83       113\n",
      "           1       0.81      0.45      0.57        65\n",
      "\n",
      "    accuracy                           0.76       178\n",
      "   macro avg       0.78      0.69      0.70       178\n",
      "weighted avg       0.77      0.76      0.74       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[106   7]\n",
      " [ 36  29]]\n",
      "\n",
      "Validation Results:\n",
      "Model: logreg_lr=0.01_lambda=10_batch=32_l2Reg\n",
      "Accuracy: 0.68\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.98      0.80       113\n",
      "           1       0.83      0.15      0.26        65\n",
      "\n",
      "    accuracy                           0.68       178\n",
      "   macro avg       0.75      0.57      0.53       178\n",
      "weighted avg       0.73      0.68      0.60       178\n",
      "\n",
      "Confusion Matrix:\n",
      "[[111   2]\n",
      " [ 55  10]]\n"
     ]
    }
   ],
   "source": [
    "# Probar 10 configuraciones para cada modelo\n",
    "\n",
    "# Árbol de Decisión\n",
    "for max_depth in range(3, 13):\n",
    "    train_decision_tree(X_train, y_train, X_val, y_val, max_depth, experiment_log=\"bitacora.csv\")\n",
    "\n",
    "# SVM\n",
    "for C in [0.1, 0.5, 1, 2, 5, 10, 20, 50, 100, 200]:\n",
    "    train_svm(X_train, y_train, X_val, y_val, C=C, kernel='rbf', gamma='scale', experiment_log=\"bitacora.csv\")\n",
    "\n",
    "# Naive Bayes (variando laplace)\n",
    "for laplace in range(1, 11):\n",
    "    train_naive_bayes_vectorized(X_train, y_train, X_val, y_val, laplace=laplace, experiment_log=\"bitacora.csv\")\n",
    "\n",
    "# Regresión Logística (variando lambda)\n",
    "for lambda_ in [0.001, 0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10]:\n",
    "    train_logistic_regression_numpy(X_train, y_train, X_val, y_val, lr=0.01, lambda_=lambda_, epochs=1000, batch_size=32, reg_type='l2', experiment_log=\"bitacora.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a9b04",
   "metadata": {},
   "source": [
    "# Ensemble Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7960d836",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test.drop(columns=[target])\n",
    "y_test = df_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "65962a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo de cada técnica:\n",
      "                     model                                  model_file  \\\n",
      "1            Decision Tree                          decisiontree_4.pkl   \n",
      "38     Logistic Regression  logreg_lr=0.01_lambda=5_batch=32_l2Reg.npy   \n",
      "20  Naive Bayes Vectorized                       naivebayesV_laplace=1   \n",
      "11                     SVM        svm_C=0.5_kernel=rbf_gamma=scale.pkl   \n",
      "\n",
      "    accuracy  precision    recall  f1_score  \n",
      "1   0.780899   0.777302  0.780899  0.776946  \n",
      "38  0.758427   0.768052  0.758427  0.737482  \n",
      "20  0.758427   0.763116  0.758427  0.760168  \n",
      "11  0.786517   0.783464  0.786517  0.781044  \n"
     ]
    }
   ],
   "source": [
    "# Cargar la bitácora\n",
    "bitacora = pd.read_csv('bitacora.csv')\n",
    "\n",
    "# Encontrar el mejor modelo por técnica según accuracy\n",
    "mejores = bitacora.loc[bitacora.groupby('model')['accuracy'].idxmax()]\n",
    "\n",
    "print(\"Mejor modelo de cada técnica:\")\n",
    "print(mejores[['model', 'model_file', 'accuracy', 'precision', 'recall', 'f1_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "69045772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Test Results:\n",
      "accuracy: 0.7932960893854749\n",
      "precision: 0.9148936170212766\n",
      "recall: 0.5657894736842105\n",
      "f1: 0.6991869918699187\n"
     ]
    }
   ],
   "source": [
    "# Cargar la bitácora y encontrar el mejor modelo de cada técnica\n",
    "bitacora = pd.read_csv('bitacora.csv')\n",
    "mejores = bitacora.loc[bitacora.groupby('model')['accuracy'].idxmax()]\n",
    "\n",
    "# Prepara los datos de test\n",
    "X_test = df_test.drop(columns=['passenger_survived'])\n",
    "y_test = df_test['passenger_survived']\n",
    "\n",
    "# Diccionario para almacenar predicciones\n",
    "predicciones = {}\n",
    "\n",
    "# Árbol de Decisión\n",
    "modelo_arbol = joblib.load(mejores.loc[mejores['model'] == 'Decision Tree', 'model_file'].values[0])\n",
    "predicciones['arbol'] = modelo_arbol.predict(X_test)\n",
    "\n",
    "# SVM\n",
    "modelo_svm = joblib.load(mejores.loc[mejores['model'] == 'SVM', 'model_file'].values[0])\n",
    "predicciones['svm'] = modelo_svm.predict(X_test)\n",
    "\n",
    "# Naive Bayes\n",
    "priors, likelihoods = load_naive_bayes_model(mejores.loc[mejores['model'] == 'Naive Bayes Vectorized', 'model_file'].values[0])\n",
    "predicciones['bayes'] = predict_naive_bayes(X_test, priors, likelihoods, laplace=1)\n",
    "\n",
    "\n",
    "# Regresión Logística\n",
    "model_file = mejores.loc[mejores['model'] == 'Logistic Regression', 'model_file'].values[0]\n",
    "w = np.load(f\"{model_file}\")\n",
    "\n",
    "predicciones['log'] = predict_logistic_regression(X_test, w)\n",
    "\n",
    "# Ensemble Voting (mayoría)\n",
    "from scipy.stats import mode\n",
    "\n",
    "for key in predicciones:\n",
    "    predicciones[key] = np.array(predicciones[key]).flatten().astype(int)\n",
    "\n",
    "y_ensemble = np.stack([\n",
    "    predicciones['arbol'],\n",
    "    predicciones['svm'],\n",
    "    predicciones['bayes'],\n",
    "    predicciones['log']\n",
    "], axis=0)\n",
    "\n",
    "from scipy.stats import mode\n",
    "y_pred = mode(y_ensemble, axis=0).mode.flatten()\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Ensemble Test Results:\\naccuracy: {acc}\\nprecision: {prec}\\nrecall: {rec}\\nf1: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0bd39",
   "metadata": {},
   "source": [
    "# K-fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f5a14",
   "metadata": {},
   "source": [
    "K-Fold Cross-Validation es una técnica para evaluar modelos dividiendo los datos en K partes (folds). El modelo se entrena K veces, usando un fold diferente como validación en cada iteración. Al final, se promedian los resultados para tener una evaluación más confiable. Tambien permitiendo multithreding.\n",
    "\n",
    "Aplicación:\n",
    "Se pudo usar para comparar mejor los modelos y reducir el sesgo de usar un solo conjunto de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013748a7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
